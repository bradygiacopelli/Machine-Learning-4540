{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/cyneuro/Neural-Networks-Machine-Learning/blob/main/stats/Maximum_Likelihood_Estimation.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "This notebook assumes the data is a standard continuous normal distribution. The number of random samples, mean, and standard deviation of the distribution can be set in lines 7-9 in code cell 1.\n",
    "\n",
    "The function `gaussian` calculates the negative log likelihood given the random samples and the initial predicted mean and standard deviations defined in initParams.\n",
    "\n",
    "The key to the whole program is the scipy function `minimize`. It is a regular optimization function and more information about it can be found [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html).\n",
    "\n",
    "**Note:** the method can be a lot of different options, all different mathematical optimization techniques.\n",
    "\n",
    "\n",
    "#### Questions:\n",
    "1. If everything is working, we would expect more accurate $\\mu^*$ and $\\sigma^*$ predictions with more random samples. Think about why this is.\n",
    "2. What happens when the initial parameter guesses are extremely off? What about when they're extremely close? How does this impact the number of samples?\n",
    "3. Break down each line in the `gaussian` function. Knowing what you know about MLE, what do you think the function `stats.norm.logpdf` does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "np.random.seed(1)\n",
    "\n",
    "# Define the true parameters for Logistic distribution\n",
    "samples = 100\n",
    "mu = 2\n",
    "s = 1.5\n",
    "\n",
    "# Generate sample data from Logistic distribution\n",
    "sample_data = np.random.logistic(loc=mu, scale=s, size=samples)\n",
    "\n",
    "\n",
    "# Negative log likelihood function for Logistic distribution\n",
    "def logistic_nll(params):\n",
    "    mean, scale = params[0], params[1]\n",
    "\n",
    "    if scale <= 0:  # Ensure scale parameter remains positive\n",
    "        return np.inf\n",
    "\n",
    "    # Calculate negative log likelihood\n",
    "    nll = -np.sum(stats.logistic.logpdf(sample_data, loc=mean, scale=scale))\n",
    "\n",
    "    return nll\n",
    "\n",
    "\n",
    "# Initial parameter guess\n",
    "initParams = [1, 1]\n",
    "\n",
    "# Perform MLE using scipy.optimize.minimize\n",
    "results = minimize(logistic_nll, initParams, method='Nelder-Mead')\n",
    "\n",
    "# Extract estimated parameters\n",
    "mu_est, s_est = results.x\n",
    "\n",
    "print('Estimated Parameters:')\n",
    "print(f'mean*  = {mu_est:.3f}, scale*  = {s_est:.3f}')\n",
    "print('True Parameters:')\n",
    "print(f'mean   = {mu:.3f}, scale   = {s:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the logistic distribution fit\n",
    "x = np.linspace(min(sample_data), max(sample_data), 100)\n",
    "y = stats.logistic.pdf(x, loc=mu, scale=s)  # True logistic distribution\n",
    "\n",
    "# Histogram of the sample data\n",
    "num_bins = 20\n",
    "plt.hist(sample_data, bins=num_bins, density=True,\n",
    "         alpha=0.6, label=\"Sampled Data\")\n",
    "\n",
    "# Plot the true logistic distribution\n",
    "plt.plot(x, y, label=\"True Logistic PDF\", linewidth=2)\n",
    "\n",
    "# Plot the estimated logistic distribution\n",
    "y_est = stats.logistic.pdf(x, loc=mu_est, scale=s_est)\n",
    "plt.plot(x, y_est, linestyle=\"dashed\",\n",
    "         label=\"Estimated Logistic PDF\", linewidth=2)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Logistic Distribution MLE Fit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "\n",
    "1. With more data, the law of large numbers ensures that the sample mean and variance converge to the true population parameters, leading to a more accurate MLE estimates for the mean and std.\n",
    "\n",
    "2. If the initial guess is extremely far off, the optimizer will take longer to converge but also run the risk of getting stuck in a local minima, or fail. On the other hand, extremely close guesses will lead to faster convergence. The larger sample size the more robust the MLE is to poor initial guesses.\n",
    "\n",
    "3. I assume that the stats.norm.logpdf function computes the log probability density of each point to get the log likelyhood, that can then be minimized in the scipy minimize function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
