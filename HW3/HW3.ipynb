{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "### #1\n",
    "\n",
    "Done on paper, line with a slope of -2 and x2 intercept at -1. Captures most of the points correctly except one, the point at (2,-1) is on the right side of the line rather than the left.\n",
    "\n",
    "### #2\n",
    "\n",
    "The parameter vector is (1,2,1). We have the equation x2 = -2x1 - 1. to match the general linear equation we can write this as 2x1 + x2 + 1 = 0. And paying attention to the order we get our parameter vector of (1,2,1).\n",
    "\n",
    "### #3\n",
    "\n",
    "It misses a point (2,-2) so the classes aren't separated very well for this small\n",
    "\n",
    "### #4\n",
    "\n",
    "The line x2 = -1x1 + .5 would capture the classes perfectly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Plot completed on paper,\n",
    "4 points are classified with the initial params of -0.5, 0, 1.\n",
    "Cost function ~1.418\n",
    "To get the optimal params we could perform a gradient descent to reduce the cost function to a minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Plots on paper again.\n",
    "\n",
    "The first plot misclassifies a lot of the points. Where Case B correctly classifies all of them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "ΘTX=w1x1+w2x2+w3x3+b\n",
    "\n",
    "Substitute the values in for wi,xi, and b and the result is 0.87\n",
    "Looking at the example below the calculation is the same as taking the transformed matrix w dot product with x.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "\n",
    "Using weights w = [1,1] and b = -1.5 we can create the proper AND threshold so that only when x1 = 1 and x2 = 2 we get an output of 1.\n",
    "For OR we need a b value of -0.5 to have the result where only 1 x1 or x2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "\n",
    "To implement the XOR Perceptron we need to layer AND, OR, NOT perceptrons.\n",
    "The first layer contains an AND Perceptron with weight w = [1,1] and bias b = -1.5, as well as an OR Perceptron with weight w = [1,1] and b = -0.5\n",
    "Second layer: NOT(AND()) Perceptron that negates the output of AND, this has weight w = [-1] and b = 0.5, this will invert the resulting AND.\n",
    "Final Layer AND(NOT(AND), OR) Perceptron with weight w [1,1] and b = -1.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7\n",
    "\n",
    "This method of layered perceptrons works because it introduces non linearity by combining multiple gates, making it possible to solve XOR.\n",
    "\n",
    "for example take the input [1,1]\n",
    "The AND == 1 (z=w1x1+w2x2+b=(1)(1)+(1)(1)−1.5=2−1.5=0.5) since z > 0 we get output 1\n",
    "OR == 1 since our output is > 0 again.\n",
    "We compute NOT(AND) next: z=−AND(x1,x2)+b=−1+0.5=−0.5 The output is 0 since z <= 0.\n",
    "Next compute AND(NOT(AND), OR): z = w1x1 + w2x2 + b = 1*0 + 1*1 - 1.5 = -0.5\n",
    "So our final XOR(1,1) = 0 since z <= 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For \"Then derive the update rule by hand for the logistic regression problem as explained at the end of the slides - just repeat what is at the end of the slides and add details that may be missing, i.e., make sure every step is listed and well undestood by you. Then develop a Notebook tutorial for logistic regression similar to what you did for linear regression.\"\n",
    "I think the important takeaway and difference between log regression and linear regression are that obviously logistic tries to predict a class, specifically 0 or 1. It does this using a sigmoid function rather than the linear function.\n",
    "The Loss function is slightly different as well using the Cross-Entropy Loss that incorporates some log functions, but is close to the same.\n",
    "The Gradient Descent Updates are largely the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sigmoid function\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Logistic regression cost function\n",
    "\n",
    "\n",
    "def J(w, b, X, y):\n",
    "    m = len(y)\n",
    "    z = np.dot(X, w) + b\n",
    "    predictions = sigmoid(z)\n",
    "    cost = -(1/m) * np.sum(y * np.log(predictions) +\n",
    "                           (1 - y) * np.log(1 - predictions))\n",
    "    return cost\n",
    "\n",
    "# Gradient of the cost function\n",
    "\n",
    "\n",
    "def gradient(w, b, X, y):\n",
    "    m = len(y)\n",
    "    z = np.dot(X, w) + b\n",
    "    predictions = sigmoid(z)\n",
    "    dw = (1/m) * np.dot(X.T, (predictions - y))\n",
    "    db = (1/m) * np.sum(predictions - y)\n",
    "    return dw, db\n",
    "\n",
    "\n",
    "# Generate simple dataset (binary classification)\n",
    "np.random.seed(42)\n",
    "X = np.array([[0.5], [1.0], [1.5], [2.0], [2.5], [3.0]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "# Initialize parameters\n",
    "w = np.zeros(X.shape[1])  # Single feature\n",
    "b = 0\n",
    "alpha = 0.5\n",
    "iterations = 1000\n",
    "\n",
    "cost_values = [J(w, b, X, y)]  # Track cost\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(iterations):\n",
    "    dw, db = gradient(w, b, X, y)\n",
    "    w = w - alpha * dw\n",
    "    b = b - alpha * db\n",
    "    cost = J(w, b, X, y)\n",
    "    cost_values.append(cost)\n",
    "    print(f\"Iteration {i+1}: w = {w[0]:.5f}, b = {b:.5f}, J(w, b) = {cost:.5f}\")\n",
    "\n",
    "# Plot cost over iterations\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(len(cost_values)), cost_values, label=\"Cost\", marker='o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"J(w, b)\")\n",
    "plt.title(\"Cost Function Minimization via Gradient Descent\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same folder as this notebook will be attached some pictures for the work done on Paper\n",
    "Thank you!\n",
    "\n",
    "For #2 on the homework it is in the other notebook XOR_FCN\n",
    "\n",
    "- Brady Giacopelli\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
