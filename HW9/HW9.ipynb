{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #1\n",
    "\n",
    "Answer the following questions: (i) When would you use MAE over MSE as a loss function for a regression problem? and (ii) What is the difference between Negative Log-Likelihood loss (NLL) and Cross Entropy loss (CE)? (Hint: check expected inputs for both functions in the PyTorch docs).  \n",
    "\n",
    "i. Mean Absolute Error (MAE) is preferred over Mean Squared Error (MSE) when the model needs to be less sensitive to outliers. Because MAE uses the absolute difference, it treats all errors linearly, whereas MSE squares the errors, which disproportionately penalizes large deviations. If your data contains noise or outliers and you want a more robust measure that doesn't let those dominate the loss, MAE is a better choice. However, MAE can be harder to optimize since its gradient is not smooth at zero.\n",
    "\n",
    "ii. NLL loss and Cross Entropy loss are closely related, but they differ in how they expect inputs. NLL loss requires the model to output log-probabilities (typically after applying a log_softmax), while Cross Entropy loss combines both log_softmax and NLL into one function. In practice, this means CE is more convenient and numerically stable since you don’t have to manually compute the log probabilities. Both are used for classification tasks, but CE is generally preferred for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #2\n",
    "In other ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #3\n",
    "image attached in folder in repo.\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is a type of neural network where each neuron in one layer is connected to every neuron in the next layer. It processes input data through a series of matrix multiplications, biases, and nonlinear activations. In image classification, the pixel values from an image are flattened into a 1D vector and fed into the MLP. Through training, the weights of the connections adjust to capture patterns that help classify the image.\n",
    "\n",
    "However, MLPs have limitations when applied to image data. They do not account for the spatial structure of images — they treat every pixel independently and ignore local patterns like edges or textures. This leads to a very large number of parameters and inefficient learning. CNNs address this by using convolutional layers that preserve spatial relationships and reduce parameter count, making them much more effective for visual tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
