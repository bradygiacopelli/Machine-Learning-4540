{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decision and Classification Trees\n",
    "- Decision trees classify data by asking yes/no questions about features.\n",
    "\n",
    "- They use Gini Impurity or Entropy to measure how “mixed” the labels are in a node.\n",
    "\n",
    "- The goal is to reduce impurity as much as possible with each split.\n",
    "\n",
    "- The tree grows by choosing the feature that results in the largest impurity reduction.\n",
    "\n",
    "- Splitting continues until:\n",
    "\n",
    "    - A node is pure (only one label), or\n",
    "\n",
    "    - A stopping condition is met (e.g. maximum depth or minimum samples).\n",
    "\n",
    "- In the example from the video:\n",
    "\n",
    "    - The task is to predict whether someone liked a movie.\n",
    "\n",
    "    - The features are whether they got popcorn or soda.\n",
    "\n",
    "    - One branch (Popcorn = Yes) has 3 \"Yes\" and 1 \"No\"\n",
    "\n",
    "    - The other branch (Popcorn = No) has 2 \"Yes\" and 2 \"No\"\n",
    "\n",
    "    - Gini impurity is calculated for each group to determine the best split.\n",
    "\n",
    "2. Regression Trees\n",
    "- Used when the target is a continuous number, not a class label.\n",
    "\n",
    "- Instead of Gini or Entropy, it uses Mean Squared Error (MSE) to measure impurity.\n",
    "\n",
    "- The tree splits the data to minimize the variance of the target variable in each node.\n",
    "\n",
    "- The final prediction at each leaf is the average of the values in that leaf.\n",
    "\n",
    "- Conceptually works the same as classification trees but uses different math to evaluate splits.\n",
    "\n",
    "3. Random Forests\n",
    "- A random forest builds many decision trees using different random subsets of data and features.\n",
    "\n",
    "- Each tree is trained on a bootstrap sample — random sampling with replacement.\n",
    "\n",
    "- At each split, a random subset of features is used instead of all features.\n",
    "\n",
    "- Each tree gives a prediction:\n",
    "\n",
    "    - For classification: majority vote across all trees\n",
    "\n",
    "    - For regression: average of all predictions\n",
    "\n",
    "- Random forests reduce overfitting by averaging across many trees.\n",
    "\n",
    "- While individual trees may be overfit, the ensemble tends to generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree diagrams are in this folder"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
