{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92144da4",
   "metadata": {},
   "source": [
    "### #1 GAN Summary\n",
    "\n",
    "- GANs consist of two competing neural networks: a generator (G) and a discriminator (D).\n",
    "\n",
    "- The generator learns to map random noise to data samples, aiming to mimic the real data distribution.\n",
    "\n",
    "- The discriminator learns to distinguish between real data and data generated by G.\n",
    "\n",
    "- Training is a minimax game: G tries to fool D, while D tries to correctly identify real vs. fake samples.\n",
    "\n",
    "- The objective function reaches equilibrium when G produces data indistinguishable from the real data, and D outputs 0.5 for all inputs.\n",
    "\n",
    "- In practice, G is updated to maximize log(D(G(z))) for better gradient flow early in training.\n",
    "\n",
    "- No inference or Markov chains are requiredâ€”sampling is done via forward passes.\n",
    "\n",
    "- GANs can generate sharp, realistic samples, but training can be unstable and requires careful coordination between G and D.\n",
    "\n",
    "- Extensions include conditional GANs, semi-supervised learning, and encoder-decoder variants for inference."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
